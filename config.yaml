debug: true
host: "0.0.0.0"
port: 5000

# Base paths and settings
ollama_path: "/data/shared/ollama_model"  # Path to Ollama repository
HF_path: "/home/xuan/cache/huggingface/hub" # Path to HuggingFace cache
HF_MIRROR: "https://hf-mirror.com" # HuggingFace mirror
docker_image: "ghcr.io/ggml-org/llama.cpp:server-cuda" # Docker image

# Timeouts and intervals
inactivity_timeout: 300  # seconds (5 minutes)
inactivity_check_interval: 60  # seconds (1 minute)
container_startup_timeout: 60  # seconds (1 minutes)
health_check_interval: 2  # seconds
request_timeout: 1800  # seconds (30 minutes)

# GPU settings
force_gpu_id: null  # 0/1/null

# docker name prefix
docker_name_prefix: "llama.cpp_scheduler"

# Common model configuration
common_config:
  # common for all models
  n_gpu_layers: 100
  threads: 76 # value copied from ollama
  device_id: null  # Will be assigned dynamically
  port: null  # Will be assigned dynamically
  # for chat completion
  predict: 2048 # default predict length

# Model specific configurations
# Use the Ollama model format: [namespace/]name[:tag] -- default tag is latest
# For example: qwen2.5:14b, llama3:8b.
# If you want to specify some other names, you can add other_names: ["other_name1", "other_name2"]. Then you can also use other_name1 or other_name2 to start the model.
models:
  qwen2.5:14b: 
    ctx_size: 80920
    parallel: 20
    flash_attn: 1
    cont_batching: 1
    chat_template: "chatml"
    other_names: ["qwen2.5-14b"]
  
  rouge/nanbeige2-16b-chat:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    chat_template: "llama3"
    other_names: ["nanbeige-16b", "nanbeige2-16b-chat"]

  phi4:14b:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    chat_template: "phi4"

  qwen2.5:7b:
    ctx_size: 80920
    parallel: 20
    flash_attn: 1
    cont_batching: 1
    chat_template: "chatml"
    other_names: ["qwen2.5-7b"]

  qwen2.5-coder:7b:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    chat_template: "chatml"
    other_names: ["qwen2.5-coder-7b"]

  deepseek-coder:6.7b:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    chat_template: "deepseek"
    other_names: ["deepseek-coder-6.7b"]

  glm4:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    chat_template: "chatglm4"
    other_names: ["glm4"]

  llama3.1:8b:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    chat_template: "llama3"
    other_names: ["llama3.1-8b"]

  all-minilm:l6-v2:
    model_ctx_size: 256
    tokenizer_from_HF: 'sentence-transformers/all-MiniLM-L6-v2' # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
    ctx_size: 256
    parallel: 1 # no need to parallel for embedding model, the increase is minimal
    flash_attn: 0 # no need for embedding model
    cont_batching: 0 # no need for embedding model
    other_names: ["all-minilm"]

  nomic-embed-text:v1.5:
    model_ctx_size: 8192
    tokenizer_from_HF: 'bert-base-uncased' # https://huggingface.co/nomic-ai/nomic-embed-text-v1.5
    ctx_size: 8192
    parallel: 1 # no need to parallel for embedding model, the increase is minimal
    flash_attn: 0 # no need for embedding model
    cont_batching: 0 # no need for embedding model
    # rope_scaling: "yarn" # not sure, do not remove it yet
    # rope_freq_scale: "0.25" # not sure, do not remove it yet
    other_names: ["nomic-embed-text"]

# Add this to your existing config.yaml
proxies:
  http: null
  https: null 