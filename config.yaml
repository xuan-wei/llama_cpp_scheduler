debug: false
host: "0.0.0.0"
port: 5000

# Base paths and settings
ollama_path: "/data/shared/ollama_model"  # Path to Ollama repository
HF_path: "/home/xuan/cache/huggingface/hub" # Path to HuggingFace cache
HF_MIRROR: "https://hf-mirror.com" # HuggingFace mirror
docker_image: "ghcr.io/ggml-org/llama.cpp:server-cuda" # Docker image

# Timeouts and intervals
inactivity_timeout: 300  # seconds (5 minutes)
inactivity_check_interval: 60  # seconds (1 minute)
container_startup_timeout: 60  # seconds (1 minutes)
health_check_interval: 2  # seconds
request_timeout: 1800  # seconds (30 minutes)

# GPU settings
force_gpu_id: null  # 0/1/null

# docker name prefix
docker_name_prefix: "llama.cpp_scheduler"

# Common model configuration
common_config:
  # common for all models
  n_gpu_layers: 100
  threads: 76 # value copied from ollama
  device_id: null  # Will be assigned dynamically
  port: null  # Will be assigned dynamically
  # for chat completion
  predict: 2048 # default predict length

# Model specific configurations
  # Example
  # qwen2.5:14b-long-context:  # custom name for qwen2.5:14b
  #   ctx_size: 65536
  #   parallel: 8
  #   flash_attn: 1
  #   cont_batching: 1
  #   ollama_name: "qwen2.5:14b" # ollama model name, [namespace/]name[:tag] -- default tag is latest
  #   other_names: ["qwen2.5:14b-long-context", "qwen2.5-14b-long-context"] # other names for the model

models:
  qwen2.5:14b: 
    ctx_size: 81920
    parallel: 20
    flash_attn: 1
    cont_batching: 1
    ollama_name: "qwen2.5:14b"
    other_names: ["qwen2.5-14b-short-context", "qwen2.5-14b", "qwen2.5:14b"]
  
  qwen2.5:14b-long-context: 
    ctx_size: 65536
    parallel: 8
    flash_attn: 1
    cont_batching: 1
    ollama_name: "qwen2.5:14b" 
    other_names: ["qwen2.5:14b-long-context", "qwen2.5-14b-long-context"]

  qwen2.5:0.5b:
    ctx_size: 81920
    parallel: 20
    flash_attn: 1
    cont_batching: 1
    ollama_name: "qwen2.5:0.5b"
    other_names: ["qwen2.5-0.5b"]
  
  rouge/nanbeige2-16b-chat:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    ollama_name: "rouge/nanbeige2-16b-chat"
    other_names: ["nanbeige-16b", "nanbeige2-16b-chat"]

  phi4:14b:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    ollama_name: "phi4:14b"
    other_names: ["phi4-14b"]

  qwen2.5:7b:
    ctx_size: 81920
    parallel: 20
    flash_attn: 1
    cont_batching: 1
    ollama_name: "qwen2.5:7b"
    other_names: ["qwen2.5-7b"]

  qwen2.5-coder:7b:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    ollama_name: "qwen2.5-coder:7b"
    other_names: ["qwen2.5-coder-7b"]

  deepseek-coder:6.7b:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    ollama_name: "deepseek-coder:6.7b"
    other_names: ["deepseek-coder-6.7b"]

  deepseek-r1:7b: # deepseek-r1-distill-qwen
    ctx_size: 81920
    parallel: 20
    flash_attn: 1
    cont_batching: 1
    ollama_name: "deepseek-r1:7b"
    other_names: ["deepseek-r1-7b"]
  
  deepseek-r1:8b: # deepseek-r1-distill-Llama
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    ollama_name: "deepseek-r1:8b"
    other_names: ["deepseek-r1-8b"]

  deepseek-r1:14b:
    ctx_size: 81920
    parallel: 20
    flash_attn: 1
    cont_batching: 1
    ollama_name: "deepseek-r1:14b"
    other_names: ["deepseek-r1-14b"]

  deepseek-r1:14b-long-context:
    ctx_size: 65536
    parallel: 8
    flash_attn: 1
    cont_batching: 1
    ollama_name: "deepseek-r1:14b"
    other_names: ["deepseek-r1:14b-long-context", "deepseek-r1-14b-long-context"]

  glm4:
    ctx_size: 40960
    parallel: 10
    flash_attn: 1
    cont_batching: 1
    ollama_name: "glm4"
    other_names: ["glm4"]

  llama3.1:8b:
    ctx_size: 40960
    parallel: 10  
    flash_attn: 1
    cont_batching: 1
    ollama_name: "llama3.1:8b"
    other_names: ["llama3.1-8b"]

  all-minilm:l6-v2:
    model_ctx_size: 256
    tokenizer_from_HF: 'sentence-transformers/all-MiniLM-L6-v2' # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
    ctx_size: 256
    parallel: 1 # no need to parallel for embedding model, the increase is minimal
    flash_attn: 0 # no need for embedding model
    cont_batching: 0 # no need for embedding model
    ollama_name: "all-minilm:l6-v2"
    other_names: ["all-minilm"]

  nomic-embed-text:v1.5:
    model_ctx_size: 8192
    tokenizer_from_HF: 'bert-base-uncased' # https://huggingface.co/nomic-ai/nomic-embed-text-v1.5
    ctx_size: 8192
    parallel: 1 # no need to parallel for embedding model, the increase is minimal
    flash_attn: 0 # no need for embedding model
    cont_batching: 0 # no need for embedding model
    rope_scaling: "yarn" 
    rope_freq_scale: "0.75" 
    ollama_name: "nomic-embed-text:v1.5"
    other_names: ["nomic-embed-text"]

# Add this to your existing config.yaml
proxies:
  http: null
  https: null 